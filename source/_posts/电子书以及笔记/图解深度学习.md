---
title: 读书笔记|图解深度学习
tags:
 - 读书笔记
 - ML
date: 2021-11-01 16:31:01
categories:
 - 读书笔记
---



# 图解深度学习

作者：山下隆义

深度学习的起源包括感知器和玻尔兹曼机，

起源于感知器的深度学习属实有监督学习，

起源于受限玻尔兹曼机的深度学习属于无监督学习 

## 神经网络

### M-P模型

需要人为设定参数，

### 感知器

通过误差修正确定参数

### 多层感知器

误差修正只能3层网络，修正中间层到输出层，不能修正输入层到中间层

### 误差反向传播算法

#### 梯度下降法

梯度消失导致无法调整连接权重问题，需要在训练过程中调整学习率n，以防止梯度消失。

### 误差函数和激活函数

#### 误差函数

1多分类：交叉熵

2二分类：

3递归问题：最小二乘误差函数

等等

#### 激活函数

对输入信号进行线性或非线性 变换

sigmoid 

tanh

ReLU

#### 似然函数

计算多层感知器的输出结果

多分类问题softmax作为似然函数

### 随机梯度下降法

根据训练样本的输入方式不同，有以下分类

1.批量学习算法 batch learning ，每次迭代计算时遍历全部训练样本

2.在线学习 sequential learning / online learning 每输入一个训练样本就进行一次迭代，可能会出现大幅变动

3.**小批量梯度下降法mini-batch learning**（随机梯度下降法）将训练集分成几个子集D，每次迭代使用一个子集

#### 学习率

首先设定一个较大值，然后慢慢减小

自适应调整学习率例如AdaGrad，AdaDelta，动量方法

## 卷积神经网络

输入层、卷积层、池化层、全连接层、输出层

### 卷积层

卷积核的通道数必须跟输入层通道数保持一致，计算是各通道分别相乘然后相加。形成1个卷积后特征图

因此卷积后特征通道数跟卷积核的个数有关，卷积核有多少个，输出通道就有多少个

零填充

填充大小P=(F-1)/2,其中F是卷积核大小

卷积结果需要经过激活函数

### 池化层

减少卷积层产生的特征图的尺寸。

最大池化，平均池化，Lp池化

### 参数设定

与神经网络有关主要参数

- 卷积层的卷积核大小、个数
- 激活函数的种类
- 池化函数的种类
- 网络的层结构（卷积层的个数、全连接层的个数）
- 全连接层的个数
- Dropout的概率
- 有无预处理
- 有无归一化

与训练有关参数

- Mini-Batch大小
- 学习率
- 迭代次数
- 有无预训练

> 首先确定重要参数，然后再对其他参数微调

## 受限玻尔兹曼机

### Hopfield神经网络

神经网络两大类：1.多层神经网络。

2.相互连接型网络：网络不分层，单元之间相互连接，联想记忆，：通过对事物之间建立对应关系来记忆。

相互连接网络可以通过联想记忆去除输入数据的噪声。

Hopfield是最典型的相互连接网络

易出现串扰（相互干扰，不能准确记忆的情况），可以采用模式正交化，但也不能完全解决。要用 玻尔兹曼机

### 玻尔兹曼机

 通过让每个单元按照一定的概率分布发生状态变化，来避免陷入局部最优解。

与Hopfield相比，两者最大的区别是Hopfield神经网络的输出是按照某种确定性决定的，而玻尔兹曼机的输出则如下所示，是按照某种概率分布决定的。

隐藏单元，可见单元

### 受限玻尔兹曼机

含有隐藏变量的玻尔兹曼机训练十分困难，加入“层内单元之间无连接”提出受限玻尔兹曼机（由可见层和隐藏层构成的两层结构）

可见层与隐藏层相互连接，但相同层内单元无连接。

计算量过于庞大问题，提出对比散度算法

### 对比散度算法

与Gibbs采样一样，对比散度算法是一种近似算法，通过较少的迭代次数求出参数调整值。

### 深度信念网络

受限玻尔兹曼机堆叠形成，与多层神经网络最大区别是训练方法不同，采用对比散度算法，逐层调整连接权重和偏置。

即可生成模型来用，也可判别模型来用

生成模型：能够去除输入数据中含有的噪声，得到新的数据，进行输入数据压缩和特征表达。

判别模型：在模型顶层添加一层来达到分类功能

## 自编码器

### 自编码器

一种有效的数据维度压缩算法，主要应用在以下两个方面

- 构建一种能够重构输入样本并进行特征表达的神经网络
- 训练多层神经网络时，通过自编码器训练样本得到参数初始值

基于无监督学习的神经网络，通过不断调整参数，重构经过维度压缩的输入样本。

### 降噪自编码器

向原本的训练样本中加入随机噪声再输入给输入层。

- 保持输入样本不变条件下，提取能够更好反映样本属性的特征。
- 消除输入样本中包含的噪声

### 稀疏自编码器

通过增加 正则化项，大部分单元的输出都变为了0，这样就能利用少数单元有效完成压缩或重构。

### 栈式自编码器

逐层训练，

## 提高泛化能力的方法

### 训练样本

数据增强：当数据集包含的数据有限或者需要自己采集但采集到的数据量又不足时，

### 预处理

- 均值减法
- 均一化
- 白化

#### 均值减法

大规模物体识别

**x**' = 1/n sum(**x**)

x~ = x - x'

各数据平均值变为0，图像整体亮度变化得到抑制。

#### 均一化

为样本的均值和方差添加约束的一种预处理方法。

先计算标准差，然后进行均值减法后，除以标准差，得到均值0，标准差1的标准化数据。

亮度差异更小

#### 白化

消除数据间相关性方法。经过白化处理后，数据之间相关性较低，图像边缘增强。

（ZCA白化）

### 激活函数

sigmoid

maxout

ReLU衍生函数

Leaky ReLU

Parametric ReLU(PReLU)

Randomized leaky Rectified Linear Units(RPeLU)

### Dropout

提高网络泛化能力方法，指：在网络的训练过程中，按照一定的概率将一部分中间层的单元暂时从网络中丢弃，通过把该单元的输出设置为0使其不工作，来避免过拟合

### DropConnect

Dropout是把单元的输出值设置为0，DropConnect是把一部分连接权重设置为0
